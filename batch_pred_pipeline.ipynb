{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9fbd8d-b04c-4ff0-b2cb-7fae64e16477",
   "metadata": {},
   "source": [
    "# Create Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdfbc24-b1c0-4ad2-a15d-4e18d69389a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "SUBSCRIPTION=<subscription id>\n",
    "RESOURCE_GROUP=<name of resource group>\n",
    "WS_NAME=<Name of Workspace>\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WS_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe3166d-904f-4f3c-b38f-18ef6cb31e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "northeurope : from_model_to_production\n"
     ]
    }
   ],
   "source": [
    "# Verify that the handle works correctly.\n",
    "ws = ml_client.workspaces.get(WS_NAME)\n",
    "print(ws.location, \":\", ws.resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4eb18c-262a-435b-b9ea-dba3e28b1c3a",
   "metadata": {},
   "source": [
    "# Create component 1: data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9263f4-e3b7-4a46-8907-48461b035ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_prep_src_dir = \"./components_pred/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7f4cc2-c69c-45dd-9c79-aeb6292cc5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components_pred/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    data_dir = args.data\n",
    "    folder_paths = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "    images = []\n",
    "    file_names = []  # List to store file names\n",
    "\n",
    "    for folder in folder_paths:\n",
    "        image_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "        for img_file in image_files:\n",
    "            img = Image.open(img_file)  # Open image using PIL\n",
    "            img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)  # Convert to BGR format\n",
    "            img = cv2.resize(img, (64, 64))  # Resize image to 64x64 pixels\n",
    "            img = img.flatten()  # Flatten the image array into a single row of pixel data\n",
    "\n",
    "            images.append(img)\n",
    "            file_names.append(os.path.basename(img_file))  # Save the image file name\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    file_names = np.array(file_names)  # File names as an array\n",
    "\n",
    "    # Split the data into training and testing datasets (without labels)\n",
    "    X_train, X_test, file_names_train, file_names_test = train_test_split(\n",
    "        images, file_names, test_size=0.2, train_size=0.8, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create DataFrames for training and testing data\n",
    "    train_df = pd.DataFrame({\n",
    "        \"images\": list(X_train),\n",
    "        \"file_name\": file_names_train  # Add the file names\n",
    "    })\n",
    "\n",
    "    test_df = pd.DataFrame({\n",
    "        \"images\": list(X_test),\n",
    "        \"file_name\": file_names_test  # Add the file names\n",
    "    })\n",
    "\n",
    "    # Save the train and test datasets as pickle files\n",
    "    train_df.to_pickle(os.path.join(args.train_data, \"train_data.pkl\"))\n",
    "    test_df.to_pickle(os.path.join(args.test_data, \"test_data.pkl\"))\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f59f21-1b72-40bd-8a5d-5226967379b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_proc_img_pred\",\n",
    "    display_name=\"This pipeline tries to register the csv as data sset\",\n",
    "    description=\"reads a URI_FOLDER input, split the input to train and test\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=data_prep_src_dir,\n",
    "    command=\"\"\"python data_prep.py \\\n",
    "            --data ${{inputs.data}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=\"aml-scikit-learn@latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b465b7d6-9a44-40ec-ba74-f898b8283de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_proc_img_pred with Version 2024-09-29-17-12-11-2339491 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df21a89-18b6-4951-9b55-39b7882ebb35",
   "metadata": {},
   "source": [
    "# Create component 2: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fed58cb-0a09-4b52-bf78-7a6beeb0bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "predict_src_dir = \"./components_pred/predict\"\n",
    "os.makedirs(predict_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2594901a-dcda-45d6-959e-2d810318844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components_pred/predict/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {predict_src_dir}/predict.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "SUBSCRIPTION=<subscription id>\n",
    "RESOURCE_GROUP=<name of resource group>\n",
    "WS_NAME=<Name of Workspace>\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WS_NAME,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the prediction script.\"\"\"\n",
    "\n",
    "    # Parse input arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, help=\"Registered model name\")\n",
    "    parser.add_argument(\"--model_version\", type=str, help=\"Model version\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"Path to the test data (without labels)\")\n",
    "    parser.add_argument(\"--predictions_output\", type=str, help=\"Path to save the predictions CSV file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start MLFlow run\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Load the registered model\n",
    "    model_uri = f\"models:/{args.model_name}/{args.model_version}\" if args.model_version else f\"models:/{args.model_name}/latest\"\n",
    "    print(f\"Loading model from {model_uri}\")\n",
    "    model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "    # Load the test data (preprocessed images)\n",
    "    test_data = pd.read_pickle(os.path.join(args.test_data, \"test_data.pkl\"))\n",
    "    X_test = np.stack(test_data[\"images\"].values)  # Convert image data to numpy array\n",
    "    file_names = test_data[\"file_name\"].values  # Get corresponding file names\n",
    "\n",
    "    # Make predictions\n",
    "    print(f\"Making predictions on test data of shape {X_test.shape}\")\n",
    "    y_pred_proba = model.predict_proba(X_test)  # Get prediction probabilities\n",
    "\n",
    "    # Prepare output DataFrame (file name and predicted probabilities)\n",
    "    predictions_df = pd.DataFrame(data=y_pred_proba, columns=[f\"label_{i}_prob\" for i in range(y_pred_proba.shape[1])])\n",
    "    predictions_df.insert(0, \"file_name\", file_names)  # Insert file name column as the first column\n",
    "\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    output_csv_path = os.path.join(args.predictions_output, \"predictions.csv\")\n",
    "    predictions_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Path to the data we just downloaded\n",
    "    data_path = output_csv_path\n",
    "\n",
    "    # Version for dataset we are creating\n",
    "    data_version = \"1\"\n",
    "\n",
    "    offline_data_asset = Data(\n",
    "        name=\"animal_predictions\",\n",
    "        version=data_version,\n",
    "        description=\"This the result of the prediction pipeline\",\n",
    "        path=data_path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "    )\n",
    "\n",
    "    # Create data asset on Azure ML\n",
    "    online_data_asset = ml_client.data.create_or_update(offline_data_asset)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "    # End MLFlow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10edc880-fec5-4ca5-ac9f-24d03b4d3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "predict_component = command(\n",
    "    name=\"predict_animal_img\",\n",
    "    display_name=\"Predictions for Animal Image Classification\",\n",
    "    description=\"This component creates a CSV that contains predictions for different images\",\n",
    "    # Inputs: test data and model name, and optionally model version\n",
    "    inputs={\n",
    "        \"test_data\": Input(type=\"uri_folder\"),\n",
    "        \"model_name\": Input(type=\"string\"),\n",
    "    },\n",
    "    # Output: path to save the predictions\n",
    "    outputs=dict(\n",
    "        predictions_output=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder containing the prediction script\n",
    "    code=predict_src_dir,  # Replace with your actual path if necessary\n",
    "    command=\"\"\"python predict.py \\\n",
    "            --test_data ${{inputs.test_data}} \\\n",
    "            --model_name ${{inputs.model_name}} \\\n",
    "            --predictions_output ${{outputs.predictions_output}}\"\"\",\n",
    "    environment=\"aml-scikit-learn@latest\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea1f942-660f-4e64-af9a-f7919f92469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading predict (0.0 MBs): 100%|██████████| 3158/3158 [00:00<00:00, 642554.19it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component predict_animal_img with Version 2024-09-29-17-12-13-7599307 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "predict_component = ml_client.create_or_update(predict_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {predict_component.name} with Version {predict_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf54f8-dc31-49c3-bc7c-da35c0164151",
   "metadata": {},
   "source": [
    "# Create the pipeline from components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79766fcf-8053-4a8e-82ae-897a002b3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=\"MLprodComputeCheap\",\n",
    "    description=\"Prediction Pipeline on animal images. this verion creates a datasset with the CSV\",\n",
    ")\n",
    "def animal_img_prediction_pipeline(\n",
    "    pipeline_job_data_input,  # The input data (images)\n",
    "    pipeline_job_registered_model_name,  # The registered model name\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        data=pipeline_job_data_input,\n",
    "    )\n",
    "\n",
    "    # using predict_component to predict using the pre-trained model\n",
    "    predict_job = predict_component(\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using test_data from data_prep_job\n",
    "        model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Returning predictions output from the predict_job\n",
    "    return {\n",
    "        \"predictions_output\": predict_job.outputs.predictions_output,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a555f3a7-de79-4e47-936a-5ab86e7a07fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the registered model name\n",
    "registered_model_name = \"image_classification_model\"\n",
    "\n",
    "# Define the image data input (your URI path)\n",
    "img_data = <Azure Path to Blob storage to read data from>\n",
    "\n",
    "# Let's instantiate the prediction pipeline with the parameters of our choice\n",
    "pipeline = animal_img_prediction_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_folder\", path=img_data),  # Data input\n",
    "    pipeline_job_registered_model_name=registered_model_name,  # Model name\n",
    "    # Optionally, you can pass model version if needed (defaults to 'latest')\n",
    "    # pipeline_job_model_version=\"1\",  # Uncomment this if you want a specific version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf998b8f-5215-4a07-abc0-7e4816448852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: nice_music_9mb8zjzb21\n",
      "Web View: https://ml.azure.com/runs/nice_music_9mb8zjzb21?wsid=/subscriptions/28cf8bdb-6861-4764-a3b5-46cfa8cf8d81/resourcegroups/from_model_to_production/workspaces/from_model_to_production\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-09-29 17:12:19Z] Submitting 1 runs, first five are: 960caeba:7922e8c9-a15e-416d-9135-a8bfbb9bdcc7\n",
      "[2024-09-29 17:12:21Z] Completing processing run id 7922e8c9-a15e-416d-9135-a8bfbb9bdcc7.\n",
      "[2024-09-29 17:12:22Z] Submitting 1 runs, first five are: d59c7d0a:385c455b-ad91-4a55-9cfc-404c47e2ae72\n",
      "[2024-09-29 17:12:50Z] Completing processing run id 385c455b-ad91-4a55-9cfc-404c47e2ae72.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: nice_music_9mb8zjzb21\n",
      "Web View: https://ml.azure.com/runs/nice_music_9mb8zjzb21?wsid=/subscriptions/28cf8bdb-6861-4764-a3b5-46cfa8cf8d81/resourcegroups/from_model_to_production/workspaces/from_model_to_production\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"animal_img_prediction\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
